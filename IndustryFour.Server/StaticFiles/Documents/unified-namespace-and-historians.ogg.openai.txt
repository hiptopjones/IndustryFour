Why or how are the UNS and a historian tightly coupled together? So, I have Justin here who is one of our new team members. He has a master's in computer science. He's focused on data science and machine learning and AI applications here at, well, at Intellic Integration. He sent me a message this morning on Teams. So, this goes to show that even some of the people who work here who join our team have questions about UNS. So, we're going to answer the question and we'll publish it to the channel. So, he said, hey, Walker, I have a question about the UNS. Justin's been here for a few weeks now. Given that data is valuable and storage is cheap, why is the historian not tightly integrated with the UNS? My intuition would be that a system that cached or otherwise maintained fast read-write the current values while also being a store for past values and state would be better than a non-integrated historian as it would provide a single source of truth for both past and current data. Because as far as I understand, you will essentially always want a historian, but it's typically a separate service. So, correct. You will always want a historian. I can't think of an industry 4.0 application in digital transformation of unified namespace where a historian or multiple historians are involved. So, in general, there are really three types of historians that are leveraged in a unified namespace architecture. So, I'm going to ask you to clarify your question here in a second, Justin, but I want to answer that last piece, which is this. So, generally, there is an edge-level historian. So, generally, there is a historian that is an off-the-shelf solution, whether that's OSI Pi, whether that's Wonderware Historian, whether that's Canary Labs, whether they're running InfluxDB out on the edge. But there's generally a historian at the plant to collect process data and be able to at least trend data over time using typical time series aggregation methods. Okay? There's also generally, as part of a digital maturity journey, there's going to end up being an enterprise-level historian. Now, whether that enterprise-level historian is the centerpiece of a federated architecture where I've got many historians on the edge, and then they're connected to an enterprise historian, and that's called federation, whether you have a federated historian at the enterprise level, or whether what I'm doing is integrating together many disparate historians out on the edge into one common historian at the enterprise level, you generally will end up with an enterprise historian. And then the last type of historian is not really a historian, but it serves the function of historical time series analysis, and that is a data lake. So, it is very common to end up having a data lake in the cloud that is then basically where time series events, all events are just dumped. And you can turn that into a historian by selecting, pulling the data from the data lake in a time series format and then putting it on a chart. But with that, go ahead and, A, where did this question come from? Number one. And number two, go ahead and clarify what you put here, and then I'll go ahead and answer it completely. So, the question came from me just thinking about, like, I saw that we had, we were showing an application, I can't say much. We were showing something, and it was mentioned that, hey, we could look at the UNS, and I was thinking, they also discussed, well, the UNS is not for historical data. That's what the historian is for, which is, you know, makes sense given the name. What that got me thinking is, why are we treating those as separate things when intuitively all the UNS is, right, would be a view of, like, the latest value in the historian? It would, it would be in, let's say, let's say we wanted to build a unified namespace just out of historical data. So, let's say that what we were going to do is the only node in our ecosystem would be a historian, okay? So, what I've got over here is a historian, okay? And all that is, is records, historical records of data points, okay? So, there's going to be two things in the historian, okay? Number one, there's going to be, well, we could either call it an asset model, okay, or a master model, okay? And all that is, is going to be, how do I get to a data point? I mean, do we, do we put historical data in a historian just flat, like there's basically a, you know, one tag, I get one temperature one, and I get one temperature two, and one temperature three, and I just drop all of those into a historian flat, or can I, or do I organize them in some way that makes sense? So, historians generally use a semantic hierarchy to store historical data, okay? Now, let's say that what we wanted to do was create a unified namespace out of the stuff that's in a historian. So, we would do two things. Number one, we would consume the asset model from the historian to give us our semantic hierarchy. So, in this case, let's say the asset model is based on ISA 95, and what I've got is enterprise, and then I have site, it could be any semantic hierarchy, but we'll use ISA 95 because everyone speaks this language, and then we have area, and then we have line, and then we have our process value. So, whatever that process value is, this is the thing that we're getting the, we're getting the historical record from to build a unified namespace. So, what would happen is we would call, or the historian would publish, this structure, and then we would select top, or we would limit one, okay? All right, there are scenarios where we do this, okay? There are scenarios where what we're doing is we're selecting top or limiting one for a value from the historian. We are also saying, go get me all the values for this process variable from the historian, query them from the historian, bring them back into the unified namespace, and the payload is actually a data set. Generally, what we do is we turn that into a JSON, it's a dictionary, okay? And what you have is either a key value pair, you have a key value pair for every column in every row, and then you have a list that equals a row. So, you have a list of key value pairs that makes up a row, and the payload is actually being consumed from the historian. That's one way we do this. If we wanted to pull it just from the historian, say I had a Canary Labs historian that's got an asset model, okay? I could go to the historian, I could go to the historian and I could consume the model, the asset model in Canary Labs historian, and I could consume limit one or top one of every process variable that I wanted from my historian. Now, I've never seen a scenario where you build the single source of truth for a business just from the historian. Why? Because there's many elements of the business that never ever get to the historian. Things like the model number, the model ID of the asset itself, say in the ERP system, or what's the asset ID from the CMMS, or what is the OEE? The OEE calculation most of the time makes its way to the historian, it ends up getting stored historically. So, one of the things that we wanted to solve with the unified namespace was to historize things that typically are not historized, okay? So, what we decided to do, and this happened 20 years ago, I did this in a salt mine using dynamic data exchange through Excel, is we turned this into a node, okay, in the digital ecosystem, all right? So, to answer your question, here's what we do. So, I'm going to go ahead and using our common technology, and I'm actually going to shoot a video that shows what our unified namespace looks like here at 4.0 Solutions and Intelliq. In fact, our unified namespace is actually 40-sol Intelliq, those are the two root nodes, but all the data and information, the single source of truth for both businesses is in one place. If I want to look at who the current staff are, what this engineer worked on today, or what was the latest time entry they put in the time management system, what is a list of all the invoices that are currently open for our clients, what's the address of this location, who is the person who's the department head of enter in said, you know, some department, all of that is inside of our unified namespace. And then what we have is we have an event engine, that the unified namespace is the event engine, and then what we have is a, we actually have a, we do a different way, we actually use a program that we wrote that converts an MQTT topical namespace into time series data, and when every event changes, we actually insert that into a historian. And so, and the definition or the ID of that value is its semantic hierarchy. So if it's, you know, enterprise site area line, which is, you'll notice that's in red, okay, enterprise site area line in notice in red, this is the semantic hierarchy of our business, okay? But then what we do is we create functional namespaces, okay? And this function could be process, which underneath process contains all the inputs and outputs for the process and the context. So if what I wanted to do was take a raw value A and multiply it by raw value B to create information value C, I do that within this process namespace, okay? And you would have access to A, B, and the results C out there. Now anybody could have access to A, B, and C just from the namespace. Moreover, by anybody, that node out here, which we call historian. Is also getting access. That also has access to it. So now what happens is as these events change, we stream them here. So if this is canary, for example, say it's canary historian, even OSI.py does this. OSI.py has a MQTT connector that where it can ingest historical data. It's a little more complicated to do it with OSI.py. That's why you want to do it through HiByte. What you would do is take, you put HiByte, you know, basically monitoring here. And then HiByte is the one that gets it to the historian. But canary labs historian supports the technology natively. So you don't need, you could still benefit from using a tool like HiByte to do data ops before you send the data into the historian, but you don't need it. So canary labs can consume this natively. And it can store the data in the structure that it was creating. Now they can do that at the area level. It can do it at the, you could do it where I have a historian that's for the area, or I have a historian that's for the site, or I have a historian for the enterprise. Does it support changing structure over time? Like we might drop a site, we might get in a site. Moreover, once that site's gone, the history is still there because we're using the semantic hierarchy to store it. But let's say the name of the asset that, you know, the line is, you know, line one has only ever existed one time. And we take line one offline on June 1st of 2024. Well, what happens is just natively by virtue of this architecture, you will have all the history of line one up until June 1st of 2024. And then you'll just have empty, nothing after that, because it stopped streaming the history. But let's say I added line two on June 1st, 2024. Line one's history would end on June 1st, 2024. And line twos would just natively start. We call this a self-aware ecosystem. I want to talk about one other thing. Okay, so historians aren't just consumers of the data or a subscriber of the data from a unified namespace. They're also producers. So this connection produces some type of information, okay? So the connection itself, the size of the, so the size, performance, the CPU on that history, all that stuff is data that comes from the historian. So let's say this historian is an area-level historian. Well, then what you're going to have here is a function called historian, where these values are published from the historian back into that space. Are they then absorbed back to the historian or is there? Then they're then absorbed back to the historians. All right, so what follows here in the rest of the video is an explanation of how we at Intellic Integration, 4.0 Solutions, and with our other integrator partners, unify our data and information. So we actually use a unified namespace for my companies. And 4.0 Solutions, which does education and outreach, and Intellic Integration, which does systems integration, they are actually architected together. They're separate companies completely, but they are two links in the same supply chain. And so where 4.0 Solutions outfeeds at the end of digital transformation maturity assessment, where it's doing the assessment of an organization, scoring, strategy, architecture, minimum technical requirements, there is then a handoff to an integrator partner. Sometimes the integrator partner is Intellic Integration. Sometimes it's GIS. Sometimes it's Skellig. Sometimes it's COPAR. Sometimes it's G5. Sometimes it's EctoBot. It could be any integrator partner, okay? They are links in the same supply chain. You hear me talk about this quite a bit. So what I actually talk about in the remaining part of the video is I'm literally talking about how two companies are integrated together on a common digital infrastructure, okay? And this is a really good example. While we don't organize our business enterprise site area line cell, we do use business functions. And it is the functions that are integrated together across two businesses. I may create a function, which is for the data and information that needs to be shared between the two companies. And that's what the rest of this video discusses. So what we got is our 40 sol. We have our Intellic. And then what we have is a server that's running Kafka from Apache. Kafka consumes all the events from our unified namespace. They just stream, okay? Kafka streams them and then stores them into Hadoop. And Hadoop is a data lake from Apache. How do we handle all their history? We do two things. We either take Hadoop and we use like ETL and we go Hadoop back to a MySQL database. And then we consume to our visualizations and we consume directly to our visualizations and we consume directly to our visualizations. Okay. Right? So depending upon the type of, this is a thing that I get all the time about unified namespace. There's lots of different types of data. There was actually this guy who like really freaked out on a LinkedIn post and he's like, you know, you've got all various different types of data. You can't use the unified namespace. Well, yes, you can. I mean, so what I have here are, what I have here is just purely transactional data. This is transactional plus time series. This is event driven. Those are your three types of data. Okay. Any other questions about? I think that answers all the questions. All right. So let me ask you this. So since you're new here, right? And because your role is different, you're a software developer, but you're not necessarily an engineer. You're in a completely new role that we have here, which is we're taking all of the digitally mature organizations that we have and we're turning you loose on machine learning use cases, right? That's the reason you're here. Because we've been doing this type of architecture with our clients now since 2015, we have 230 companies that are extremely digitally mature and they have the right infrastructure now where you have the data you need in the context you need it normalized, cleansed and ready for ingestion into algorithms. We're turning you loose on that, but we haven't given you the same training that we give our engineers, right? So you're coming in totally blind and just whatever you can learn on YouTube, we're giving you as little guidance as humanly possible and kind of seeing where you go. What have you learned about, from the moment you interviewed here till now, what is it that you've learned about this architecture that stands out to you? Good, bad and ugly. I think, well, principally good is that it's all there. I've seen a lot of context and a lot of problems where there'll be people will want to learn from something. They'll want to model some data or the world or whatever. And the first problem they have to solve is to record it. And if you haven't been doing that, it's actually a really hard problem. But especially if you, it's also a big blocker because you might just, if you start recording now, how long will it be before you have enough data for it to be useful? If you start recording, it's like that saying, the best time to plant a tree is five years ago, the second best time is today. Even though there's a lot of time between five years and to go today, but that's beside the point. This is where the old, this is where our truism, all data matters. Yeah, exactly. Make assumptions about how data will be consumed. So I think that assumption makes my life a lot easier and a lot faster because I don't have to be, because I have in the past had to be like, all right, well now, not only do I need to figure out how to learn for the data, I actually have to get the data. The data's already out there already in a usable format and I've got it all historicized and that's good. That's really, that makes my life so much easier. And it's got the, because the chronological data is essential because if you don't know how the state of all, or how the state of the system or collection of systems rather evolves, then you can't learn anything from it. Let me ask you this. So when you were completing your master's, so he has a master's degree in computer science. Your undergraduate degree is in? Also computer science. The master's was just focused on AI and machine learning. So your master's is in artificial intelligence and machine learning. Primarily the stuff that you did in grad school was focused in the financial industry, right? No, it was some medical, some DOD. Right, medical and DOD. One of the big things that comes up here, right? That I, every client we talk to wants to do machine learning, except they generally call it, we want to do predictive analytics. We want to do predictive process control. We want to do statistical process control. One of the things that becomes abundantly clear in this industry is that the people who are writing the request for proposal for a machine learning use case have no idea what machine learning actually is. Like they have no idea what it actually takes to be able to predict the future from something I'm looking at right now, which by the way, that's what ML is. ML is using algorithms to look at something right now, a model I've trained to look at something right now to predict the future. You know how you train that? With the past and putting together the past in a format that is digestible, reliable, reliable, not missing a lot of gaps and available is incredibly difficult. This is why we say all data matters, make no assumptions about how data will be consumed. But in the cases that you were looking at, what was the biggest challenge you guys faced, you as students, as grad students, what was the biggest challenge you guys faced in using ML and artificial intelligence, well, ML to predict the future? Let's stick it, let's limit it to just machine learning. Oftentimes it was data quality, like especially in the medical field. So much of that is entered by people, often by nurses. And of course they're doing a hard job, a lot of work. And it's very easy to have just tiny little holes or where to a person, you know, to, or rather, let me rephrase, to an expert in the field. And I think it's fair to say none of us are medical doctors. To an expert in the field, it might be clear, but what belongs there, but we, and more importantly, the machine learning algorithms can't figure out those holes effectively without a huge amount of data, at least some of which needs to be pretty good. So with, if ML is your goal, predicting the future by monitoring the present, trained on the past, okay? Is this architecture coupled with a historian, which was your question, right? Because we don't actually talk about the historian that much in our content. Is this what you want? Exactly. It's exactly what you want. Yeah. Historical data with a high quality, good source of truth, context, everything. Awesome. All right. And did I answer your question? Yes, sir. All right, awesome. Hopefully I answered your question. So like, subscribe, comment down below. Thank you for watching and I'll see you in the next one.